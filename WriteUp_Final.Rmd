---
title: "Predicting and Classifying Human Activity"
author: "Arjun Jeyapaalan"
date: "Friday, August 21, 2015"
output: html_document
---

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

## Dataset

The training data for this project is available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).

The test data is available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).

## Goal

The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.  

## Methodology

An overall pseudo-random number generator seed was set at 1234 for all code. For the sake of reproduceability, the same seed should be used.
Various packages such as caret and randomForest  were downloaded and installed. 

### How the Model was built?

The variable that we are trying to predict (i.e.:outcome variable) is classe, a factor variable with 5 levels. For this data set, "participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in 5 different fashions:

* Class A: Exactly according to the specification
* Class B: Throwing the elbows to the front
* Class C: Lifting the dumbbell only halfway
* Class D: Lowering the dumbell only halfway
* Class E: Throwing the hips to the front

Class A represents the right way of executing the exercise while the other classes represent mistakes in the execution of the exercise. Prediction evaluations will be based on maximizing the accuracy and minimizing the out-of-sample error. All other available variables after cleaning will be used for prediction.
Two models will be tested using decision tree and random forest algorithms. The model with the highest accuracy will be chosen as our final model.

### How Cross-Validation was used?

Cross-validation will be performed by subsampling our training data set randomly without replacement into 2 subsamples: subTraining data (75% of the original Training data set) and subTesting data (25%). Our models will be fitted on the subTraining data set, and tested on the subTesting data. Once the most accurate model is choosen, it will be tested on the original Testing data set.

### Expected out of sample error 

The expected out-of-sample error will correspond to the quantity: 1-accuracy in the cross-validation data. Accuracy is the proportion of correct classified observation over the total sample in the subTesting data set. Expected accuracy is the expected accuracy in the out-of-sample data set (i.e. original testing data set). Thus, the expected value of the out-of-sample error will correspond to the expected number of missclassified observations/total observations in the Test data set, which is the quantity: 1-accuracy found from the cross-validation data set.

### Reasons for the choices made

The outcome variable "classe" is an unordered factor variable. Thus, we can choose our error type as 1-accuracy. We have a large sample size with N= 19622 in the training data set. This allow us to divide the training sample into subTraining and subTesting to allow cross-validation. Features with all missing values and features that are irrelevant will be discarded.
Decision tree and random forest algorithms are known for their ability of detecting the features that are important for classification. 

## Results 

### Loading libraries, datasets and initial cleaning of data

Note: Outputs are hidden for space considerations.

```{r, results='hide'}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)

set.seed(1234)

#Loading the datasets
trainingset <- read.csv("C:/Users/Arjoon/Desktop/Course Project/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
testingset <- read.csv("C:/Users/Arjoon/Desktop/Course Project/pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))

dim(trainingset)
dim(testingset)

#Cleaning the Dataset
trainingset<-trainingset[,colSums(is.na(trainingset)) == 0]
testingset <-testingset[,colSums(is.na(testingset)) == 0]

trainingset   <-trainingset[,-c(1:7)]
testingset <-testingset[,-c(1:7)]

dim(trainingset)
dim(testingset)
head(trainingset)
head(testingset)
```

### Divide the training data set to allow cross-validation

In order to perform cross-validation, the training data set is divided into 2 sets: subTraining (75%) and subTest (25%).
This will be performed using random subsampling without replacement.

Note: Outputs are hidden for space considerations.

```{r, results='hide'}
subsamples <- createDataPartition(y=trainingset$classe, p=0.75, list=FALSE)
subTraining <- trainingset[subsamples, ] 
subTesting <- trainingset[-subsamples, ]
dim(subTraining)
dim(subTesting)
head(subTraining)
head(subTesting)
```

### Visualize the Data

A plot of the outcome variable will allow us to see the frequency of each levels of the "classe" variable in the subTraining data set and compare one another.

```{r}
plot(subTraining$classe, col="red", main="Levels of the classe variable within the subTraining data set", xlab="classe levels", ylab="Frequency")
```

From the graph above, we can see that each level frequency is within the same order of magnitude of each other. Level A (i.e.: the right movement for the activity) is the most frequent with more than 4000 occurrences while level D is the least frequent with about 2500 occurrences.

### Prediction model no.1: Using Decision Tree

```{r}
model1 <- rpart(classe ~ ., data=subTraining, method="class")
prediction1 <- predict(model1, subTesting, type = "class")

rpart.plot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
```

Test using the subTesting dataset:

```{r}
confusionMatrix(prediction1, subTesting$classe)
```

### Prediction model no.2: Using Random Forest

```{r}
model2 <- randomForest(classe ~. , data=subTraining, method="class")
prediction2 <- predict(model2, subTesting, type = "class")
```

Test using the subTesting dataset:
```{r}
confusionMatrix(prediction2, subTesting$classe)
```

### Which algorithm performed better?

Accuracy for Random Forest model was 0.995 (95% CI: (0.993, 0.997)) compared to 0.739 (95% CI: (0.727, 0.752)) for Decision Tree model. Therefore, the random forest model is choosen. The accuracy of the model is 0.995. The expected out-of-sample error is estimated at 0.005, or 0.5%. The expected out-of-sample error is calculated as 1 - accuracy for predictions made against the cross-validation set. Our test data set comprises 20 cases. With an accuracy above 99% on our cross-validation data, we can expect that very few, if any at all, of the test samples will be wrongly classified.

## Classifying the original test dataset using Random Forest Model

```{r}
predictfinal <- predict(model2, testingset, type="class")
predictfinal
```

## References 

[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

[2] Krzysztof Grabczewski and Norbert Jankowski. Feature Selection with Decision Tree Criterion.

